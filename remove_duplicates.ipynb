{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "def find_duplicates_sift(dir1, dir2, threshold=10, ratio=0.75):\n",
    "    sift = cv2.SIFT_create()\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
    "\n",
    "    def get_image_files(directory):\n",
    "        supported_formats = ('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff')\n",
    "        return [os.path.join(directory, f) for f in os.listdir(directory)\n",
    "                if f.lower().endswith(supported_formats)]\n",
    "\n",
    "    def compute_kp_desc(image_path):\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            return None, None\n",
    "        keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "        return keypoints, descriptors\n",
    "\n",
    "    dir2_files = get_image_files(dir2)\n",
    "    dir2_descriptors = []\n",
    "    for file in tqdm(dir2_files):\n",
    "        _, desc = compute_kp_desc(file)\n",
    "        if desc is not None:\n",
    "            dir2_descriptors.append((file, desc))\n",
    "\n",
    "    dir1_files = get_image_files(dir1)\n",
    "    duplicates = []\n",
    "\n",
    "    for file1 in tqdm(dir1_files):\n",
    "        _, desc1 = compute_kp_desc(file1)\n",
    "        if desc1 is None:\n",
    "            continue\n",
    "\n",
    "        for file2, desc2 in dir2_descriptors:\n",
    "            matches = bf.knnMatch(desc1, desc2, k=2)\n",
    "            good_matches = [m for m, n in matches if m.distance < ratio * n.distance]\n",
    "            if len(good_matches) >= threshold:\n",
    "                duplicates.append(file1)\n",
    "                break\n",
    "\n",
    "    return duplicates\n",
    "\n",
    "def delete_duplicates(duplicates):\n",
    "    for file in duplicates:\n",
    "        try:\n",
    "            os.remove(file)\n",
    "            print(f\"Deleted: {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {file}: {e}\")\n",
    "    print(f\"Total deleted images: {len(duplicates)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "145e47c037424fe98ed6e6165e0feb2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d4bd0f11044556967209e3a70b4625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/558 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 duplicate(s).\n",
      "No duplicates found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "directory1 = \"data_small/test/mask\"\n",
    "directory2 = \"data_small/train/mask\"\n",
    "similarity_threshold = 10\n",
    "ratio_test = 0.6\n",
    "\n",
    "duplicates_found = find_duplicates_sift(directory1, directory2, threshold=similarity_threshold, ratio=ratio_test)\n",
    "print(f\"Found {len(duplicates_found)} duplicate(s).\")\n",
    "if duplicates_found:\n",
    "    confirm = input(\"Do you want to delete these duplicates? (y/n): \").strip().lower()\n",
    "    if confirm == 'y':\n",
    "        delete_duplicates(duplicates_found)\n",
    "    else:\n",
    "        print(\"Deletion aborted.\")\n",
    "else:\n",
    "    print(\"No duplicates found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e47867d24545b0bc01ddde5e3be9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81061f08d0a4be88e3f2c4994fd0bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 122 duplicate(s).\n",
      "Deletion aborted.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "directory1 = \"data_small/test/nomask\"\n",
    "directory2 = \"data_small/train/nomask\"\n",
    "similarity_threshold = 20\n",
    "ratio_test = 0.6\n",
    "\n",
    "duplicates_found = find_duplicates_sift(directory1, directory2, threshold=similarity_threshold, ratio=ratio_test)\n",
    "print(f\"Found {len(duplicates_found)} duplicate(s).\")\n",
    "if duplicates_found:\n",
    "    confirm = input(\"Do you want to delete these duplicates? (y/n): \").strip().lower()\n",
    "    if confirm == 'y':\n",
    "        delete_duplicates(duplicates_found)\n",
    "    else:\n",
    "        print(\"Deletion aborted.\")\n",
    "else:\n",
    "    print(\"No duplicates found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
